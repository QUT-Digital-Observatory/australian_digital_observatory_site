_model: page
---
title: The ADO Ecosystem
---
_discoverable: yes
---
body:

The Australian Digital Observatory (ADO) is curating an [ecosystem of resources](../resources) for researchers working with digital human data from the internet. This ecosystem is intended to be useful to a wide range of disciplines, particularly humanities and social sciences, data science, public health, business, law, and many others.

Research projects are, by definition, all unique: the very purpose of research is to make new contributions. Therefore, there is no one-size-fits-all system for obtaining and pre-processing research data, especially in the humanities and social sciences. However, there are many overlapping methods and skills that are part of the research data lifecycle including (but not limited to) collecting, tidying, analysing, and publishing data.

The ADO ecosystem aims to equip researchers with a set of modular, open source, interoperable methods and processes to assist with these tasks. Researchers can engage with and benefit from the ADO ecosystem by accessing tools, training, and project support services.

This modular ecosystem approach gives flexibility, allowing researchers to pick and choose [resources](../resources) that are relevant and suitable for them, without being locked in to a specific stack (e.g., proprietary formats/tools, specific cloud infrastructure). The modular approach also recognises different entry levels in terms of researchers’ skills and needs. Furthermore, smaller tools are easier to maintain, and focusing on modularity and interoperability allows making use of and supporting existing tools and resources already in the community rather than reinventing the wheel.

In designing and curating the ADO ecosystem, we take inspiration from the open source software community and [the Unix philosophy](https://medium.com/ingeniouslysimple/philosophy-of-unix-development-aa0104322491) of designing interoperable tools that each do one thing well, and from foundational tooling ecosystems such as [the R Tidyverse](https://rviews.rstudio.com/2017/06/08/what-is-the-tidyverse/#:~:text=The%20tidyverse%20is%20a%20coherent,being%20expanded%20by%20several%20contributors.), which has well demonstrated the suitability of such a structure to the academic research environment. We hope to learn from these communities, and also to contribute back to them.

### Example workflows

Here are some examples of research workflows that benefit from our ecosystem approach. 

**Twitter conversation analysis**. A researcher is interested in the Twitter conversation around Australia’s federal election. The workflow for answering this problem would include a preliminary feasibility check (to ensure there are sufficient data for analysis), followed by data collection, processing, and analysis.

To implement this workflow, we use the following tools:

<ul>
<li><a href="https://www.ado.eresearch.unimelb.edu.au/tutorials/" target="_blank">ADORed</a>: high-level social media analysis via interactive dashboard. It can be used to derive a list of tweet IDs relevant to the research question.</li>
<li><a href="https://twarc-project.readthedocs.io/en/latest/twarc2_en_us/" target="_blank">twarc hydrate</a>: open-source command-line tool for extracting data from Twitter API. It can be used to hydrate full tweet content from tweet IDs.</li>
<li><a href="https://pypi.org/project/tidy-tweet/" target="_blank">tidy_tweet</a>: in-development open-source Python library for processing raw responses from Twitter API.</li>
<li>tweet_exploR: in-development R package providing descriptive statistics of Twitter data.</li>
</ul>

The chart below illustrates the research workflow, as well as the specific tools used in each phase.

<img src="example-workflow-sans.png" alt="DO example workflow for Twitter analysis" class="img-fluid rounded mx-auto d-block" >

<br> 

**Multi-platform content analysis**. Many research projects require data from multiple sources. For example, a researcher is interested in the dynamics and development of content and networks related to the Critical Race Theory in light of the Black Lives Matter movement. Analyses would require data from Twitter, Youtube, Reddit, Wikipedia, as well as scholarly citation networks. A set of different bespoke tools are developed and used to support this workflow:

<ul>
<li><a href="https://twarc-project.readthedocs.io/en/latest/twarc2_en_us/" target="_blank">twarc hydrate</a>: open-source command-line tool for extracting data from Twitter API. It can be used to hydrate full tweet content from tweet IDs.</li>
<li><a href="https://pypi.org/project/tidy-tweet/" target="_blank">tidy_tweet</a>: in-development open-source Python library for processing raw responses from Twitter API.</li>
<li><a href="/resources/research-project-support/">youtube_collector</a>: custom script developed by QUT Digital Observatory to assist with collecting and processing YouTube data (which is also currently being developed into an open-source library).</li>
<li><a href="/resources/research-project-support/">reddit_collector</a>: custom script developed by QUT Digital Observatory to assist with collecting and processing Reddit data via the open Pushshift data platform.</li>
<li><a href="/resources/research-project-support/">wikipedia_collector</a>: custom script developed by QUT Digital Observatory to assist with collecting and processing Wikipedia data.</li>
<li><a href="/resources/research-project-support/">citation_collector</a>: custom script developed by QUT Digital Observatory to assist with collecting and processing scholarly citation data from OpenAlex database.</li>
</ul>
